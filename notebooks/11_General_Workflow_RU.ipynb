{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пример общего рабочего процесса обработки осциллограмм\n",
    "\n",
    "Этот ноутбук демонстрирует возможную последовательность шагов при обработке коллекции осциллограмм с использованием ранее созданных модульных классов. Каждый этап может быть адаптирован, пропущен или выполнен независимо в соответствии с конкретными задачами.\n",
    "\n",
    "**Этапы рабочего процесса:**\n",
    "1.  **Обнаружение и сбор осциллограмм**: Используем `OscillogramFinder` для поиска и копирования файлов нужных типов из исходной директории (включая архивы) в рабочую директорию.\n",
    "2.  **Анонимизация**: Применяем `DataAnonymizer` для удаления конфиденциальной информации и переименования файлов по хеш-сумме.\n",
    "3.  **Каталогизация имен сигналов**: Используем `SignalNameManager` для создания каталога всех имен сигналов. (Опционально: их последующее переименование).\n",
    "4.  **Фильтрация по активности**: Применяем `OscillogramActivityFilter` для отсеивания \"пустых\" или неактивных осциллограмм.\n",
    "5.  **Генерация коэффициентов нормализации**: На основе активных осциллограмм создаем файл `norm.csv` с помощью `NormalizationCoefficientGenerator`.\n",
    "6.  **Преобразование в CSV**: Конвертируем обработанные (анонимизированные, активные, нормализованные) осциллограммы в формат CSV с помощью `OscillogramToCsvConverter` для дальнейшего анализа или использования в ML.\n",
    "7.  **Специализированный анализ (пример)**: Краткая демонстрация запуска одного из анализаторов, например, `SPEFAnalyzer` или `OvervoltageDetector`.\n",
    "\n",
    "Все пути к файлам и директориям, а также конфигурации, являются примерами и должны быть адаптированы под реальные задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "import zipfile \n",
    "import sys\n",
    "\n",
    "# Настройка путей для импорта модулей проекта\n",
    "module_path = os.path.abspath(os.path.join(os.getcwd(), '..')) \n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from core.oscillogram import Oscillogram\n",
    "from filesystem.finder import OscillogramFinder, TYPE_OSC\n",
    "from preprocessing.anonymizer import DataAnonymizer\n",
    "from preprocessing.signal_names import SignalNameManager\n",
    "from analysis.activity_filter import OscillogramActivityFilter, ChannelType \n",
    "from normalization.normalization import NormalizationCoefficientGenerator, OscillogramNormalizer\n",
    "from raw_to_csv.raw_to_csv import OscillogramToCsvConverter \n",
    "from analysis.spef import SPEFAnalyzer \n",
    "from analysis.overvoltage_detector import OvervoltageDetector \n",
    "\n",
    "base_workflow_dir = \"temp_workflow_demonstration\"\n",
    "initial_source_dir = os.path.join(base_workflow_dir, \"00_initial_source\")\n",
    "staging_dir_found = os.path.join(base_workflow_dir, \"01_staging_found_oscillograms\")\n",
    "staging_dir_anonymized = os.path.join(base_workflow_dir, \"02_staging_anonymized\")\n",
    "staging_dir_active = os.path.join(base_workflow_dir, \"03_staging_active_oscillograms\")\n",
    "output_configs_dir = os.path.join(base_workflow_dir, \"output_and_configs\")\n",
    "\n",
    "if os.path.exists(base_workflow_dir):\n",
    "    shutil.rmtree(base_workflow_dir)\n",
    "os.makedirs(initial_source_dir)\n",
    "os.makedirs(staging_dir_found)\n",
    "os.makedirs(staging_dir_anonymized)\n",
    "os.makedirs(staging_dir_active)\n",
    "os.makedirs(output_configs_dir)\n",
    "\n",
    "print(f\"Созданы базовые директории в: {base_workflow_dir}\")\n",
    "\n",
    "def create_workflow_cfg_dat(path, name, analog_channels_data, digital_channels_data=[], freq=50.0, samp_rate=1000.0, dat_rows=50):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    cfg_content = f\"InitialStation,Device_{name},1999\\n\"\n",
    "    cfg_content += f\"{len(analog_channels_data) + len(digital_channels_data)},{len(analog_channels_data)}A,{len(digital_channels_data)}D\\n\"\n",
    "    ch_idx = 1\n",
    "    for ch_name_cfg, _ in analog_channels_data:\n",
    "        cfg_content += f\"{ch_idx},{ch_name_cfg},A,,V,1.0,0.0,0,-32767,32767,1,1,P\\n\" \n",
    "        ch_idx += 1\n",
    "    for ch_name_cfg, _ in digital_channels_data:\n",
    "        cfg_content += f\"{ch_idx},{ch_name_cfg},,,1\\n\"\n",
    "        ch_idx += 1\n",
    "    cfg_content += f\"{freq}\\n1\\n{samp_rate},{dat_rows}\\n\"\n",
    "    cfg_content += f\"01/01/2023,00:00:00.000000\\n01/01/2023,00:00:00.000000\\nASCII\\n1.0\\n\"\n",
    "    with open(os.path.join(path, f\"{name}.cfg\"), \"w\", encoding=\"utf-8\") as f: f.write(cfg_content)\n",
    "\n",
    "    dat_content = \"\"\n",
    "    for i in range(dat_rows):\n",
    "        dat_content += f\"{i+1},{int(i*(1000000/samp_rate))}\"\n",
    "        for _, data_series in analog_channels_data: dat_content += f\",{data_series[i % len(data_series)]:.6f}\"\n",
    "        for _, data_series in digital_channels_data: dat_content += f\",{data_series[i % len(data_series)]}\"\n",
    "        dat_content += \"\\n\"\n",
    "    with open(os.path.join(path, f\"{name}.dat\"), \"w\", encoding=\"utf-8\") as f: f.write(dat_content)\n",
    "\n",
    "time_wf = np.linspace(0, 2*np.pi*2.5, 50)\n",
    "active_data = np.concatenate([np.sin(time_wf[:25])*0.5, np.sin(time_wf[25:])*1.5])\n",
    "create_workflow_cfg_dat(initial_source_dir, \"wf_active_osc\", \n",
    "                        [(\"U_Active\", active_data)], \n",
    "                        [(\"ML_Event_Dummy\", np.concatenate([np.zeros(20, dtype=int),np.ones(10, dtype=int),np.zeros(20, dtype=int)]))])\n",
    "\n",
    "stable_data = np.sin(time_wf)*0.8\n",
    "create_workflow_cfg_dat(initial_source_dir, \"wf_empty_osc\", [(\"I_Stable\", stable_data)])\n",
    "\n",
    "os.makedirs(os.path.join(initial_source_dir, \"subdir_initial\"), exist_ok=True)\n",
    "create_workflow_cfg_dat(os.path.join(initial_source_dir, \"subdir_initial\"), \"wf_active_osc_sub\", \n",
    "                        [(\"U_Active_Sub\", active_data*0.7)])\n",
    "\n",
    "archive_path = os.path.join(initial_source_dir, \"archive_with_osc.zip\")\n",
    "temp_archive_files_path = os.path.join(base_workflow_dir, \"temp_for_zip\")\n",
    "os.makedirs(temp_archive_files_path, exist_ok=True)\n",
    "create_workflow_cfg_dat(temp_archive_files_path, \"temp_archived_osc\", [(\"U_Archived\", stable_data*0.3)])\n",
    "with zipfile.ZipFile(archive_path, 'w') as zf:\n",
    "    zf.write(os.path.join(temp_archive_files_path, \"temp_archived_osc.cfg\"), \"temp_archived_osc.cfg\")\n",
    "    zf.write(os.path.join(temp_archive_files_path, \"temp_archived_osc.dat\"), \"temp_archived_osc.dat\")\n",
    "shutil.rmtree(temp_archive_files_path)\n",
    "    \n",
    "def create_dummy_file(filepath, content=\"dummy_content\"):\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f: f.write(content)\n",
    "create_dummy_file(os.path.join(initial_source_dir, \"some_other_file.txt\"), \"some text\")\n",
    "\n",
    "print(f\"Содержимое initial_source_dir создано.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 1: Обнаружение и сбор осциллограмм (`OscillogramFinder`)\n",
    "Копируем только файлы COMTRADE (.cfg/.dat) из `initial_source_dir` (включая архивы) в `staging_dir_found`.\n",
    "Сохраняем структуру директорий и используем хеши для избежания дубликатов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder_wf = OscillogramFinder(is_print_message=True)\n",
    "initial_hashes_path_wf = os.path.join(staging_dir_found, \"_hash_table.json\") \n",
    "\n",
    "ft_flags = {type_enum: False for type_enum in TYPE_OSC} \n",
    "ft_flags[TYPE_OSC.COMTRADE_CFG_DAT] = True\n",
    "ft_flags[TYPE_OSC.ARCH_ZIP] = True\n",
    "ft_flags[TYPE_OSC.ARCH_7Z] = True \n",
    "ft_flags[TYPE_OSC.ARCH_RAR] = True \n",
    "\n",
    "copied_count_wf = finder_wf.copy_new_oscillograms(\n",
    "    source_dir=initial_source_dir,\n",
    "    dest_dir=staging_dir_found,\n",
    "    preserve_dir_structure=True,\n",
    "    use_hashes=True,\n",
    "    file_type_flags=ft_flags,\n",
    "    max_archive_depth=1 \n",
    ")\n",
    "print(f\"\\nЭтап 1: Скопировано {copied_count_wf} осциллограмм в {staging_dir_found}\")\n",
    "print(\"Содержимое staging_dir_found:\")\n",
    "for root, dirs, files in os.walk(staging_dir_found):\n",
    "    level = root.replace(staging_dir_found, '').count(os.sep)\n",
    "    indent = ' ' * 4 * (level)\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    sub_indent = ' ' * 4 * (level + 1)\n",
    "    for f_name in files: print(f\"{sub_indent}{f_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 2: Анонимизация (`DataAnonymizer`)\n",
    "Анонимизируем файлы в `staging_dir_found` и сохраняем их в `staging_dir_anonymized`.\n",
    "DataAnonymizer сам переименовывает файлы в хеши и кладет их в ту же директорию, где нашел.\n",
    "Поэтому для чистоты эксперимента, сначала скопируем файлы в `staging_dir_anonymized`, а потом анонимизируем их там."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(staging_dir_anonymized): shutil.rmtree(staging_dir_anonymized)\n",
    "shutil.copytree(staging_dir_found, staging_dir_anonymized, dirs_exist_ok=True) \n",
    "print(f\"Скопированы файлы из {staging_dir_found} в {staging_dir_anonymized} для анонимизации.\")\n",
    "\n",
    "anonymizer_wf = DataAnonymizer()\n",
    "anonymizer_wf.anonymize_directory(staging_dir_anonymized) # Использует print для вывода ошибок/варнингов\n",
    "\n",
    "print(f\"\\nЭтап 2: Анонимизация завершена в {staging_dir_anonymized}.\")\n",
    "print(\"Содержимое staging_dir_anonymized (файлы должны быть переименованы в хеши):\")\n",
    "for root, dirs, files in os.walk(staging_dir_anonymized):\n",
    "    level = root.replace(staging_dir_anonymized, '').count(os.sep)\n",
    "    indent = ' ' * 4 * (level)\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    sub_indent = ' ' * 4 * (level + 1)\n",
    "    for f_name in files: print(f\"{sub_indent}{f_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 3: Каталогизация имен сигналов (`SignalNameManager`)\n",
    "Создадим каталог имен сигналов из анонимизированных файлов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_wf = SignalNameManager()\n",
    "catalog_path_wf = os.path.join(output_configs_dir, \"workflow_signal_catalog.csv\")\n",
    "sm_wf.find_signal_names(staging_dir_anonymized, output_csv_path=catalog_path_wf, is_print_message=True)\n",
    "\n",
    "if os.path.exists(catalog_path_wf):\n",
    "    print(\"\\nСодержимое каталога сигналов:\")\n",
    "    try: from IPython.display import display; display(pd.read_csv(catalog_path_wf))\n",
    "    except ImportError: print(pd.read_csv(catalog_path_wf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 4: Фильтрация по активности (`OscillogramActivityFilter`)\n",
    "Отфильтруем \"пустые\" осциллограммы из анонимизированной директории. Результаты (список активных файлов) сохраним в CSV. Затем скопируем только активные файлы в `staging_dir_active`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_config_wf = {\n",
    "    'channels_to_analyze_patterns': ['U_Active', 'I_Stable', 'U_Archived'], \n",
    "    'current_channel_id_patterns': ['i_'],\n",
    "    'voltage_channel_id_patterns': ['u_'],\n",
    "    'use_norm_osc': False, \n",
    "    'raw_signal_analysis': {\n",
    "        'initial_window_check_periods': 1, \n",
    "        'h1_vs_hx_ratio_threshold_U': 2, \n",
    "        'h1_vs_hx_ratio_threshold_I': 2,\n",
    "        'min_initial_h1_amplitude_for_rel_norm': 0.005, \n",
    "        'thresholds_raw_current_relative': {'delta': 0.1, 'std_dev': 0.05, 'max_abs': 0.1},\n",
    "        'thresholds_raw_voltage_relative': {'delta': 0.1, 'std_dev': 0.05, 'max_abs': 0.1}\n",
    "    },\n",
    "    'verbose': False\n",
    "}\n",
    "activity_filter_wf = OscillogramActivityFilter(config=activity_config_wf, normalizer=None)\n",
    "active_files_report_path = os.path.join(output_configs_dir, \"workflow_active_files.csv\")\n",
    "\n",
    "activity_filter_wf.verbose = True\n",
    "activity_filter_wf.filter_directory(staging_dir_anonymized, active_files_report_path)\n",
    "activity_filter_wf.verbose = False\n",
    "\n",
    "if os.path.exists(active_files_report_path):\n",
    "    active_files_df = pd.read_csv(active_files_report_path)\n",
    "    if not active_files_df.empty and 'active_files' in active_files_df.columns:\n",
    "        print(f\"\\nКопирование {len(active_files_df)} активных файлов в {staging_dir_active}...\")\n",
    "        for _, row in active_files_df.iterrows():\n",
    "            active_cfg_name = row['active_files'] \n",
    "            for root, _, files in os.walk(staging_dir_anonymized):\n",
    "                if active_cfg_name in files:\n",
    "                    src_cfg_path = os.path.join(root, active_cfg_name)\n",
    "                    src_dat_path = os.path.join(root, active_cfg_name[:-4] + \".dat\")\n",
    "                    \n",
    "                    relative_dir = os.path.relpath(root, staging_dir_anonymized)\n",
    "                    dest_subdir_active = os.path.join(staging_dir_active, relative_dir)\n",
    "                    os.makedirs(dest_subdir_active, exist_ok=True)\n",
    "                    \n",
    "                    dest_cfg_path = os.path.join(dest_subdir_active, active_cfg_name)\n",
    "                    dest_dat_path = os.path.join(dest_subdir_active, active_cfg_name[:-4] + \".dat\")\n",
    "                    \n",
    "                    if os.path.exists(src_cfg_path): shutil.copy2(src_cfg_path, dest_cfg_path)\n",
    "                    if os.path.exists(src_dat_path): shutil.copy2(src_dat_path, dest_dat_path)\n",
    "                    break \n",
    "        print(f\"Содержимое {staging_dir_active} после копирования активных файлов:\")\n",
    "        for root, dirs, files in os.walk(staging_dir_active):\n",
    "            level = root.replace(staging_dir_active, '').count(os.sep)\n",
    "            indent = ' ' * 4 * (level); print(f\"{indent}{os.path.basename(root)}/\")\n",
    "            sub_indent = ' ' * 4 * (level + 1)\n",
    "            for f_name in files: print(f\"{sub_indent}{f_name}\")\n",
    "    else:\n",
    "        print(\"Активных файлов для копирования не найдено.\")\n",
    "else:\n",
    "    print(\"Отчет об активных файлах не создан.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 5: Генерация коэффициентов нормализации (`NormalizationCoefficientGenerator`)\n",
    "Используем активные, анонимизированные осциллограммы из `staging_dir_active` для генерации `norm.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_norm_csv_path = os.path.join(staging_dir_active, \"norm.csv\") \n",
    "\n",
    "coeff_gen_wf = NormalizationCoefficientGenerator(\n",
    "    osc_path=staging_dir_active, \n",
    "    prev_norm_csv_path=\"\", \n",
    "    bus=1 \n",
    ")\n",
    "# coeff_gen_wf.is_print_message = True # If needed\n",
    "\n",
    "print(\"\\nЗапуск генерации коэффициентов нормализации...\")\n",
    "coeff_gen_wf.normalization()\n",
    "\n",
    "if os.path.exists(workflow_norm_csv_path):\n",
    "    print(f\"\\nФайл коэффициентов '{workflow_norm_csv_path}' успешно создан.\")\n",
    "    try: from IPython.display import display; display(pd.read_csv(workflow_norm_csv_path).head())\n",
    "    except ImportError: print(pd.read_csv(workflow_norm_csv_path).head())\n",
    "else:\n",
    "    print(f\"\\nОшибка: Файл '{workflow_norm_csv_path}' не был создан.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 6: Преобразование в CSV (`OscillogramToCsvConverter`)\n",
    "Конвертируем активные, анонимизированные осциллограммы в CSV, используя сгенерированные коэффициенты нормализации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_analog_names_path = os.path.join(output_configs_dir, \"wf_dict_analog.json\")\n",
    "wf_analog_content = { \"bus1\": { \n",
    "    \"U_Active\": [\"U_Active\"], \"I_Stable\": [\"I_Stable\"], \n",
    "    \"U_Active_Sub\": [\"U_Active_Sub\"], \"U_Archived\": [\"U_Archived\"]\n",
    "} }\n",
    "with open(wf_analog_names_path, \"w\", encoding=\"utf-8\") as f: json.dump(wf_analog_content, f)\n",
    "wf_discrete_names_path = os.path.join(output_configs_dir, \"wf_dict_discrete.json\")\n",
    "wf_discrete_content = { \"bus1\": { \"ML_Event\": [\"ML_Event_Dummy\"]}}\n",
    "with open(wf_discrete_names_path, \"w\", encoding=\"utf-8\") as f: json.dump(wf_discrete_content, f)\n",
    "\n",
    "if not os.path.exists(workflow_norm_csv_path):\n",
    "    print(f\"Файл {workflow_norm_csv_path} не найден! Пропускаем этап конвертации в CSV.\")\n",
    "else:\n",
    "    normalizer_workflow = OscillogramNormalizer(norm_coef_file_path=workflow_norm_csv_path, is_print_message=True)\n",
    "\n",
    "    converter_wf = OscillogramToCsvConverter(\n",
    "        normalizer=normalizer_workflow,\n",
    "        raw_path=staging_dir_active, \n",
    "        csv_path=output_configs_dir, \n",
    "        uses_buses=['1'], \n",
    "        dict_analog_names_path=wf_analog_names_path,\n",
    "        dict_discrete_names_path=wf_discrete_names_path,\n",
    "        is_print_message=True\n",
    "    )\n",
    "    converter_wf.number_periods = 1 \n",
    "\n",
    "    final_csv_name = \"final_dataset.csv\"\n",
    "    print(f\"\\nЗапуск финального преобразования в CSV (выход: {final_csv_name})...\")\n",
    "    df_final = converter_wf.create_csv(csv_name=final_csv_name, is_cut_out_area=True, is_simple_csv=False)\n",
    "\n",
    "    if df_final is not None and not df_final.empty:\n",
    "        print(f\"\\nПервые строки итогового CSV ({final_csv_name}):\")\n",
    "        try: from IPython.display import display; display(df_final.head())\n",
    "        except ImportError: print(df_final.head())\n",
    "        print(f\"Колонки: {df_final.columns.tolist()}\")\n",
    "    else:\n",
    "        print(f\"Итоговый CSV не был создан или пуст.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 7: Специализированный анализ (Пример: `SPEFAnalyzer`)\n",
    "Это опциональный шаг. Можно запустить любой из анализаторов на полученных данных.\n",
    "Для `SPEFAnalyzer` потребовалась бы более специфичная настройка `norm.csv` и конфигурации самого анализатора.\n",
    "Здесь просто обозначим возможность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nПример вызова SPEFAnalyzer (закомментирован, требует своей конфигурации):\")\n",
    "# spef_config_wf = { \n",
    "# 'VALID_NOMINAL_VOLTAGES': {6000.0/np.sqrt(3)}, # Пример\n",
    "# 'SPEF_THRESHOLD_U0': 0.05, 'SPEF_THRESHOLD_Un': 0.03, \n",
    "# 'SPEF_MIN_DURATION_PERIODS': 1, \n",
    "# 'SIMILAR_AMPLITUDES_FILTER_ENABLED': True, \n",
    "# 'SIMILAR_AMPLITUDES_MAX_RELATIVE_DIFFERENCE': 0.15, \n",
    "# 'verbose': True, 'norm_yes_phrase': 'YES' \n",
    "# }\n",
    "# if os.path.exists(workflow_norm_csv_path):\n",
    "#     norm_coeffs_df_wf = pd.read_csv(workflow_norm_csv_path)\n",
    "#     spef_analyzer_wf = SPEFAnalyzer(\n",
    "#         config=spef_config_wf,\n",
    "#         normalizer=normalizer_workflow, # Уже есть\n",
    "#         bus_splitter=converter_wf,    # Уже есть (OscillogramToCsvConverter)\n",
    "#         norm_coef_df=norm_coeffs_df_wf\n",
    "#     )\n",
    "#     spef_report_path_wf = os.path.join(output_configs_dir, \"workflow_spef_report.csv\")\n",
    "#     spef_error_log_wf = os.path.join(output_configs_dir, \"workflow_spef_errors.log\")\n",
    "#     spef_analyzer_wf.analyze_directory(staging_dir_active, spef_report_path_wf, spef_error_log_wf)\n",
    "#     if os.path.exists(spef_report_path_wf):\n",
    "#         print(\"Отчет SPEF:\")\n",
    "#         try: from IPython.display import display; display(pd.read_csv(spef_report_path_wf))\n",
    "#         except ImportError: print(pd.read_csv(spef_report_path_wf))\n",
    "# else:\n",
    "#     print(\"Файл norm.csv не найден, анализ SPEF пропущен.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Очистка временной директории\n",
    "try:\n",
    "    if os.path.exists(base_workflow_dir):\n",
    "        shutil.rmtree(base_workflow_dir)\n",
    "        print(f\"\\nВременная директория {base_workflow_dir} удалена.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка при удалении временной директории {base_workflow_dir}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
